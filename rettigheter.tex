%S� kanskje det ikke er s� lenge til en AI blir selvbevist. Eller hva hvis man
%kan overf�re et menneskets tanker til en datamaskin.
Allerede idag har vi biler som kj�rer av seg selv og kun trenger en
menneskelig sj�f�r for � tilfredsstille eksisterende lover og det er ikke
utenkelig at det innen kort tid vil v�re lov for en bil � kj�re helt alene. I fremtiden vil du kunne starte en <<app>> p� telefonen din og be bilen din komme og hente deg.

P� veien kommer bilen ut for en ulykke og ender med � ta liv av en person.
Hvem er det som har ansvaret? Er det deg som eier av bilen, eller er
selskapet som produserte bilen? Selvom det kanskje idag ville v�rt selskapet eller eierens skyld i dag s� er det tenkelig at det i fremtiden vil komme AI-er som vil l�re ting p� egenh�nd og selskapet ikke lengre har kontroll over hva den gj�r, for eksempel en robot som tar førerprøven. P� et eller annet tidspunkt kan det hende vi m� innse at en AI har blitt et eget individ med
egen vilje. Kanskje blir det � lage en AI i fremtiden mye av det samme som � bli forelder, med ansvar og oppdragelse
Etter dette vil det v�re de individuelle AI-ene som selv har ansvaret og
skaperne kun kan f� skylden dersom AI-en er under ett visst stadie i utviklingsprosessen.

N�r AI'er har blitt beviste og f�tt samme rettigheter som mennesker - hva da
med alle industriroboter osv. Er det etisk akseptabelt � lage AI'er som er for
<<dumme>> til � v�re selvbeviste? Det er jo kanskje mer praktisk med inteligente industriroboter s� kanskje man
lager AIer som er inteligente nok til � v�re selvbeviste, men vi legger inn en
sperre slik at den ikke f�r fri vilje. Hva hvis man gj�r det samme med
mennesker? Lager mennesker med medf�dt hjerneskade som kan brukes i industri
og minerydding? 

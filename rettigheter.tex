Allerede idag har vi biler som kjører av seg selv og kun trenger en
menneskelig sjåfør for å tilfredsstille eksisterende lover og det er ikke
utenkelig at det innen kort tid vil være lov for en bil å kjøre helt alene. I fremtiden vil du kunne starte en <<app>> på telefonen din og be bilen din komme og hente deg.

På veien kommer bilen ut for en ulykke og ender md å ta liv av en person.
Hvem er det som har ansvaret? Er det deg som eier av bilen, eller er
selskapet som produserte bilen? Selvom det kanskje idag ville vært selskapet eller eierens skyld i dag så er det tenkelig at det i fremtiden vil komm AI-er som vil lære ting på egenånd og selskapet ikke lengre har kontroll ver hva den gjør, for eksempel en robot som tar førerprøven. På et eller annet tidspunkt kan det hende vi må innse at en AI har blitt et eget individ med
egen vilje. Kanskje blir det å lage en AI i fremtiden mye av det samme som å bli forelder, med ansvar og oppdragelse
Etter dette vil det våre de individuelle AI-ene som selv har ansvaret og
skaperne kun kan få skylden dersom AI-en er under ett visst stadie i utviklingsprosessen.

Når AI'er har blitt beviste og fått samme rettigheter som mennesker - hva da
med alle industriroboter osv. Er det etisk akseptabelt å lage AI'er som er for
<<dumme>> til å være selvbeviste? Det er jo kanskje mer praktisk med inteligente industriroboter så kanskje man
lager AIer som er inteligente nok til å være selvbeviste, men vi legger inn en
sperre slik at den ikke får fri vilje. Hva hvis man gjør det samme med
mennesker? Lager mennesker med medfødt hjerneskade som kan brukes i industri
og minerydding? 

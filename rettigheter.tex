%Så kanskje det ikke er så lenge til en AI blir selvbevist. Eller hva hvis man
%kan overføre et menneskets tanker til en datamaskin.
Allerede idag har vi biler som kjører av seg selv og kun trenger en
menneskelig sjåfør for å tilfredsstille eksisterende lover og det er ikke
utenkelig at det innen kort tid vil være lov for en bil å kjøre helt alene. Tenk deg at du på vei hjem fra jobben har kommet over noe du vil kjøpe, men som du ikke får med deg på
toget og at du da kan du starte en <<app>> på telefonen din og be bilen din komme og hente deg.

På veien kommer bilen ut for en ulykke og ender med å ta liv av en person.
Hvem er det som har ansvaret? Kanskje er det deg som eier av bilen, eller kanskje det er
selskapet som produserte bilen, burde de ha forutsett hva som skulle
skje? Selvom det kanskje idag ville vært selskapet eller eierens skyld i dag så er det tenkelig at det i fremtiden vil komme AI-er som vil lære ting på egenhånd og selskapet ikke lengre har kontroll over hva den gjør, for eksempel en robot som tar lappen og består på samme måte som vi mennesker. På et eller annet tidspunkt kan det hende vi må innse at en AI har blitt et eget individ med
egen vilje. Kanskje blir det å lage en AI i fremtiden mye av det samme som å bli forelder: Dersom man skal lage en AI må man gjøre det man kan for at AIen skal følge
lover og regler.
Etter dette vil det være de individuelle AI-ene som selv har ansvaret og
skaperne kun kan få skylden dersom AI-en er under ett visst stadie i utviklingsprosessen.

Når AI'er har blitt beviste og fått samme rettigheter som mennesker - hva da
med alle industriroboter osv. Er det etisk akseptabelt å lage AI'er som er for
<<dumme>> til å være selvbeviste? Det er jo kanskje mer praktisk med inteligente industriroboter så kanskje man
lager AIer som er inteligente nok til å være selvbeviste, men vi legger inn en
sperre slik at den ikke får fri vilje. Hva hvis man gjør det samme med
mennesker? Lager mennesker med medfødt hjerneskade som kan brukes i industri
og minerydding? 
